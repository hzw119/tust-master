## 决策树

本质：通过对特征属性的分类对样本进行分类的树型结构。即对**特征空间进行划分的树型结构**

组成元素：

1. 根节点：表示第一个特征属性，只有出边没有入边
2. 内部节点：表示特征属性，有一条入边至少两条出边
3. 叶子节点：表示分类结果

**loss** 是:**准确率，泛化性能**

## 特征选择ID3

定义：选择具有最佳分类效果的特征。

熵entropy的定义：信息的期望值，期望值越大，说明信息的**不确定性更大**，所含信息量最大。

熵entropy的计算公式： $H=- \sum_{i=1}^{n}p(x_i) log_2(p(x_i)) $

熵的概率由数据估计得到时，所对应的熵是**经验熵**：$H(D)=-\sum_{k=1}^{K} \frac{c_k}{D} log_2\frac{c_k}{D}$

D 表示样本容量，$c_k$指的是属于k类样本的个数

条件熵$H(Y|X)$：在已知随机条件X发生的条件下，随机变量Y的不确定性。 $H(Y|X)=\sum_{i=1}^{n} p_iH(Y|X=x_i)$        $p_i$=P(x=xi)

信息增益：划分数据集之后**信息发生的变化**称为信息增益

信息增益（互信息）：

特征A对数据集D的信息增益=D的经验熵-特征A发生的条件下D的条件经验熵

$G(D,A)=H(D)-H(D|A)$

设决策树最终的分类结果为C($c_{i....k}$)，**特征A有n个不同的取值**

H(D|A)=$\sum_{i=1}^{n}\frac{|D_i|}{|D|} (\sum_{k=1}^{K}\frac{|D_{ik}|}{|D_{i}|} log_2\frac{|D_{ik}|}{|D_{i}|})$

$D_i$:A特征取值为$A_i$的取值个数

$D_{ik}$:样本的特征A取值为$A_i$ 且 样本分类结果是$c_k$的样本个数



决策树生成算法：

**ID3：**在决策树各个结点上对应信息增益准则选择特征，递归地构建决策树

递归终止条件：1. 所有类标签相同 2.使用完了所有特征，当使用完了所有特征，仍然不能将数据划分仅包含唯一类别的分组，则说明特征不够用。

**ID3算法在某次切割了某特征时，此特征在之后不可使用，且不能处理连续性特征**

**ID3 为什么不能剪枝**



C4.5:剪枝,如果剪枝之后,整体损失降低了就剪枝,如果没有降低就不剪枝 

   C4.5：解决某一特征的类别多





# CART

CART（Classification And Regression Trees）：**二元切分法**处理连续性特征，最终生成二叉树

步骤:

1. 决策树生成：递归构建二叉决策树，选择最优特征
2. 决策树剪枝：定义损失函数，**预剪枝和后剪枝**

CART分类树的损失函数:**基尼指数**

CART回归树的损失函数:MSE

### 1.决策树生成:最小的基尼指数

1. 特征选择：CART采用**基尼系数**选择特征，基尼系数**越小**、不纯度越低，特征越好。

2. 基尼指数：$Gini(t)=1- \sum_k[p(c_t|t)]^2$

3. 父节点对应的样本集合为D，CART假定选择A分裂两个子节点，对应集合为$D_l$与$D_R$分裂后的**条件Gini指数**

   ​     G(D,A)=$\frac{D_l}{D} Gini(D_l)+\frac{D_R}{D} Gini(D_R)$

4. 遍历所有可能的$D_l$与$D_R$的分裂方法，求最小的基尼指数

5. 停止分裂条件：

   1. 样本个数小于预定阈值
   2. Gini指数小于预定阈值（样本基本属于一类）

6. 连续特征离散化:

   m个样本的连续特征A有m个，从小到大排列$a_1，a_2，......，a_m$，则CART取相邻两样本值的平均数做划分点，一共取m-1个，其中第i个划分点Ti表示为：Ti = $(a_i + a_i+1)/2$。分别计算以这m-1个点作为二元分类点时的基尼系数。选择**基尼系数最小**的点为该连续特征的二元离散分类点。比如取到的基尼系数最小的点为at，则小于at的值为类别1，大于at的值为类别2，这样就做到了连续特征的离散化。

### 2. CART分类树的剪枝

https://www.cnblogs.com/keye/p/10564914.html

剪枝目的：极小化整体的**损失函数**

损失函数： 

$L_{\alpha}(T)=C(T)+\alpha|T|$ $C(T)$:决策树的训练误差，|T| 为叶节点个数，模型的复杂度，$\alpha$为**调节参数**

$C(T)$=$\sum_{i=1}^{|T|}N_t H_t(T)$:每一个叶子结点的实例数*每一个叶子结点的经验熵

CART剪枝的具体办法：

1. 将$\alpha$ 从 [$\alpha_i$,$\alpha_{i+1}$)逐渐递增，计算得到对应于区间内的最优子树为$T_i$

2. 从最优子树序列${T_1,T_2,⋯,T_n}$选出最优的（即损失函数最小的）

   计算最优子树$T_i$:

   1. 定义以t为单节点（叶子节点）的损失函数：$L_\alpha (t)=C(t)+\alpha\left| T_t \right|$    (没有剪枝)

   2. 以t为根的子树的损失函数：$L_\alpha (T_t)=C(T_t)+\alpha$                   (剪枝之后)

      $L_\alpha (t)=L_\alpha (T_t)$ =>$\alpha = {C(t)-C(T_t) \over \left| T_t \right|-1}$=>$g(t) = {C(t)-C(T_t) \over \left| T_t \right|-1}$,g(t)为剪枝后整体**损失函数减少程度**

   剪枝流程如下：

   1. 对于输入的初始决策树$T_0$,自上而下**计算各个内部节点(非叶子节点)的g(t),**选择**最大的g(t)**(剪枝效果最大)作为α1,并进行剪枝得到树T1，其为区间[α1,α2)对应的最优子树.
   2. 迭代



# CART回归树

建立:https://www.cnblogs.com/keye/p/10601501.html

实质:特征区间的划分,划分之后的每一个特征单元有一个**固定的输出值**$c_m$

$c_m$ 其实是每一个特征单元的均值

建成方法:

![CART回归树](C:\Users\Administrator\Desktop\ms\ml\img\CART回归树.png)

决策树总结:

| 算法 | 支持模型 | 树结构     | 特征选择            | 连续值处理 | 缺失值处理 | 剪枝     |
| ---- | -------- | ---------- | ------------------- | ---------- | ---------- | -------- |
| ID3  | 分类     | 多叉树     | 信息增益            | 不支持     | 不支持     | 不支持   |
| C4.5 | 分类     | **多叉树** | 信息增益比          | 支持       | 支持       | **支持** |
| CART | 分类回归 | 二叉树     | 基尼系数/**均方差** | 支持       | 支持       | 支持     |



### 决策树的缺点：

1. 容易过拟合
2. 不稳定,对于样本的变化极为**敏感**

决策树的优点：

1. 易于理解和解释
2. 白盒模型