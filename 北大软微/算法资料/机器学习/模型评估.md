## **over-fitting and under-fitting**

1. 定义

     over-fit：模型过于复杂，把噪声数据的特征也学习到模型中。（泛化能力低）

    under-fit：没有捕捉到数据的特征，不能很好的拟合训练集和测试集

2. 避免过拟合的方法：

   避免过拟合：

      1. 获得更多的训练数据

      2. 降低模型复杂度，比如在神经网络中，减少hidden 层的个数

      3. 正则化方法：L1、L2 正则

         为什么正则化可以避免过拟合？

          初始loss  func：$C=C_{0}$

          **加了L2正则**后的loss func：$C=C_{0}+\frac{\lambda}{2n} \sum_{w}w^{2}$

         $\lambda$ 时超参，正则化系数

         计算$C$的梯度如下：

          $\frac{\partial C}{\partial w}=\frac{\partial C_{0}}{\partial w} +\frac{\lambda}{n}w$

         更新w：

         w=w-$\eta(\frac{\partial C_{0}}{\partial w} +\frac{\lambda}{n}w)$=($1-\eta\frac{\lambda}{n}$)w-$\eta \frac{\partial C_{0}}{\partial w} $

         ($1-\eta\frac{\lambda}{n}w$) <1 ：**真实效果是减小w，称为weight decay**

         w变小，模型的复杂度更低，对数据的拟合刚刚好。

         过拟合的函数波动很大，在某些很小的区间里，函数值的变化很剧烈，则函数在某些小区间里的导数值非常大，由于自变量值可大可小，所以只有系数足够大，才能保证导数值够大

       4. 集成学习方法

3. avoid 欠拟合的方法

   1. 添加新特征
   2. 增加模型的复杂度