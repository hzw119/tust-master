## 实现线性回归(AX=b)的方法：

1. 逆矩阵相乘直接得到参数：

   计算公式：防止X不满秩

     ![机器学习实战教程（十一）：线性回归基础篇之预测鲍鱼年龄](https://www.cuijiahua.com/wp-content/uploads/2017/11/ml_11_7.png)

2. 矩阵分解：
   cholesky 分解：把一个正定矩阵表示成一个**下三角矩阵L**和其**转置的**乘积的分解

   $A^{T}Ax=A^{T}b$,通过cholesky 分解将$A^{T}A=R^{T}R$    R是上三角矩阵，$R^T$ 为下三角矩阵

   AX=b=>$A^{T}Ax=A^{T}b$=>$R^{T}Rx=A^Tb$
   $$
   R^T w = A^T b \\
      R x = w
   $$
   使用线性方程组求解，来先求出w,然后再解线程方程组$R x = w$

   tensorflow代码：

   ```python
   tA_A = tf.matmul(tf.transpose(A_tensor), A_tensor) 
   L = tf.cholesky(tA_A) #分解AT A得到 L
   tA_b = tf.matmul(tf.transpose(A_tensor), b)
   sol1 = tf.matrix_solve(L, tA_b) #求解w
   sol2 = tf.matrix_solve(tf.transpose(L), sol1) #求解x
   ```

   

   ## 线性回归的种类

   ### deming regression

   相比于线性回归，deming回归的loss度量不同：

   <img src="C:\Users\Administrator\Desktop\ms\ml\img\1.png" alt="QQ截图20200327214349" style="zoom:67%;" />

   

<img src="C:\Users\Administrator\Desktop\ms\ml\img\2.png" alt="QQ截图20200327214400" style="zoom:67%;" />

###   lasso 回归、ridge 回归

lasso（套索回归）的loss:
$$
J = \frac{1}{n}\sum_{i = 1}^n (f( x_i) - y_i)^2 + \lambda \|w\|_1
\tag{3}
$$
ridge回归（岭回归）的loss：


$$
J = \frac{1}{n}\sum_{i = 1}^n (f( x_i) - y_i)^2 + \lambda \|w\|_2^2
\tag{4}
$$

权重计算公式：

![机器学习实战教程（十二）：线性回归提高篇之乐高玩具套件二手价预测](https://cuijiahua.com/wp-content/uploads/2017/12/ml_12_2.png)



矩阵**I**是一个mxm的单位矩阵，加上一个λI从而使得矩阵非奇异，λ是一个超参

ridge回归所解决的问题：**特征比样本点还多**，此时的X不满秩，防止过拟合，通过引入对W权重的惩罚项，能够减少不重要的参数，这个技术在统计学中也可以叫做缩减。

<img src="C:\Users\Administrator\Desktop\ms\ml\img\L正则化.png" alt="L正则化" style="zoom: 33%;" />



**为了使用岭回归和缩减技术，首先需要对特征做标准化处理**

参考链接：https://www.cnblogs.com/wuliytTaotao/archive/2019/05/11/10837533.html

**L1正则和L2正则的比较**

 lasso 可以做**feature selection**

###   局部加权线性回归

 目标：解决线性回归的欠拟合现象

 方法：给待预测点附近的每个点都赋予一定的权重，与knn一样，这种算法每次预测均需要事先选取出对应的数据子集。权重新的计算方法如下：

![机器学习实战教程（十一）：线性回归基础篇之预测鲍鱼年龄](https://www.cuijiahua.com/wp-content/uploads/2017/11/ml_11_12.png)

 W：是一个对角矩阵，w[i,i] 代表**当前节点**对与其他的设置权重

使用核对附近的点赋予更高的权重。核的类型可以自由选择，最常用的核就是高斯核：

![机器学习实战教程（十一）：线性回归基础篇之预测鲍鱼年龄](https://cuijiahua.com/wp-content/uploads/2017/11/ml_11_13_1.jpg)

代码如下：

```python
def lwlr(testPoint, xArr, yArr, k = 1.0):
    """
    函数说明:使用局部加权线性回归计算回归系数w
    Parameters:
        testPoint - 样本点，依据该样本点
        xArr - x数据集
        yArr - y数据集
        k - 高斯核的k,自定义参数
    Returns:
        ws - 回归系数
    """
    xMat = np.mat(xArr); yMat = np.mat(yArr).T
    m = np.shape(xMat)[0]
    weights = np.mat(np.eye((m)))                #创建权重对角矩阵
    for j in range(m):                           #遍历数据集计算每个样本的权重
        diffMat = testPoint - xMat[j, :]                                 
        weights[j, j] = np.exp(diffMat * diffMat.T/(-2.0 * k**2))
    xTx = xMat.T * (weights * xMat)                                        
    if np.linalg.det(xTx) == 0.0:
        print("矩阵为奇异矩阵,不能求逆")
        return
    ws = xTx.I * (xMat.T * (weights * yMat))                            #计算回归系数
    return testPoint * ws
```

### 前向迭代回归

一种**贪心算法**，每一步都尽可能较少误差。我们迭代iteNum此，每一次迭代中，每个特征一次减少或者增加某个小数值，改变后计算损失loss。迭代下去，取最小loss对应的权重。

```python
def stageWise(xArr, yArr, eps = 0.01, numIt = 100):
    """
    函数说明:前向逐步线性回归
    Parameters:
        xArr - x输入数据
        yArr - y预测数据
        eps - 每次迭代需要调整的步长
        numIt - 迭代次数
    Returns:
        returnMat - numIt次迭代的回归系数矩阵
     """
    xMat = np.mat(xArr); yMat = np.mat(yArr).T      #数据集
    xMat, yMat = regularize(xMat, yMat)             #数据标准化
    m, n = np.shape(xMat)
    returnMat = np.zeros((numIt, n))                #初始化numIt次迭代的回归系数矩阵
    ws = np.zeros((n, 1))                           #初始化回归系数矩阵
    wsTest = ws.copy()
    wsMax = ws.copy()
    for i in range(numIt):                          #迭代numIt次
        lowestError = float('inf');                 #正无穷
        for j in range(n):                          #遍历每个特征的回归系数
            for sign in [-1, 1]:
                wsTest = ws.copy()
                wsTest[j] += eps * sign             #微调回归系数
                yTest = xMat * wsTest               #计算预测值
                rssE = rssError(yMat.A, yTest.A)    #计算平方误差
                if rssE < lowestError:              #如果误差更小，则更新当前的最佳回归系数
                    lowestError = rssE
                    wsMax = wsTest
        ws = wsMax.copy()
        returnMat[i,:] = ws.T                      #记录numIt次迭代的回归系数矩阵
    return returnMat
```

