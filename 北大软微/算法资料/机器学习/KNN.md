## 1.距离度量

Lp距离:
$$
L_p(x_i, x_j) = \left( \sum_{l=1}^{n}
    | x_i^{(l)} - x_j^{(l)} |^p \right)^{\frac{1}{p}}
$$
p=1:曼哈顿距离：

在欧几里得空间的固定直角坐标系上两点所形成的线段对轴产生的投影的距离总和

p=2：欧式距离:
$$
d_{12}=\sum_{k=1}^n\sqrt{(x_1-x_2)^2}
$$
p->无穷大：切比雪夫距离：point1：（x1,y1）point2 (x2,y2)
$$
d_{12}=max(|x_1-x_2|,|y_1-y_2|)
$$

## 2.K值选取

1. k值较小，**学习的近似误差**会减小（只有与输入实例较近的训练实例才会对预测结果起作用）

   **预测误差**会增大，发生过拟合。 等同于模型变得复杂。

2. k值较大，**学习的近似误差**会增大，**预测误差**会减少，发生欠拟合。等同于模型变得简单。

## 3.KNN算法流程

1. 计算已知类别数据集中的点与当前点之间的距离
2. 按距离递增次序排序
3. 选取与当前点距离最小的**k**个点
4. 统计前k个点所在的类别出现的频率
5. 选择前k个点出现**频率最高的类别**作为预测类别

## 4.优缺点

advantage：

1. easy，算法复杂度**O(nlogn)**低
2. 适用大样本自动分类
3. 适合于多分类问题

disadvantage:

1. 计算量大；
2. 容易出现**维度灾难**；
3. **样本不平衡问题**（有的类别的样本数量很多、其他样本数量很少）
4. 输出可解释性不强

## 5.KNN优化-KD树

建立**KD树的流程**：

   m个样本n维特征，计算n个特征的方差，取方差最大的第k维特征作为根节点。取k维特征的中位数作为树的分裂点，小于中位数的放左子树，大于的放右子树，递归。

搜索**最近邻**：x的最近邻

1. 沿着KD树寻找对应的叶子节点x1。
2. 计算x与叶子节点x1的距离l，并且以x为中心l为半径画超球体。
3. 寻找超球体与x距离最小的点为最近邻x2

KD树划分后可以**大大减少无效的最近邻搜索**，很多样本点由于所在的超矩形体和超球体不相交，根本不需要计算距离。大大节省了计算时间。